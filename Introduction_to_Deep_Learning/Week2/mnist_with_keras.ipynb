{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# MNIST digits classification with Keras\n",
    "\n",
    "We don't expect you to code anything here because you've already solved it with TensorFlow.\n",
    "\n",
    "But you can appreciate how simpler it is with Keras.\n",
    "\n",
    "We'll be happy if you play around with the architecture though, there're some tips at the end."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/mnist_sample.png\" style=\"width:30%\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We're using TF 1.2.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We are using Keras 2.0.6\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "import tensorflow as tf\n",
    "print(\"We're using TF\", tf.__version__)\n",
    "import keras\n",
    "print(\"We are using Keras\", keras.__version__)\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"../..\")\n",
    "import keras_utils\n",
    "from keras_utils import reset_tf_session"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Look at the data\n",
    "\n",
    "In this task we have 50000 28x28 images of digits from 0 to 9.\n",
    "We will train a classifier on this data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import preprocessed_mnist\n",
    "X_train, y_train, X_val, y_val, X_test, y_test = preprocessed_mnist.load_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train [shape (50000, 28, 28)] sample patch:\n",
      " [[ 0.          0.29803922  0.96470588  0.98823529  0.43921569]\n",
      " [ 0.          0.33333333  0.98823529  0.90196078  0.09803922]\n",
      " [ 0.          0.33333333  0.98823529  0.8745098   0.        ]\n",
      " [ 0.          0.33333333  0.98823529  0.56862745  0.        ]\n",
      " [ 0.          0.3372549   0.99215686  0.88235294  0.        ]]\n",
      "A closeup of a sample patch:\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPgAAAD8CAYAAABaQGkdAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAACTFJREFUeJzt3U9onAUexvHnMVup0AUPnUNpyqYHEYqwCqFIeysIVYte\nFRQPQi8rVBBEPQhePHgQL16K/xYURdCDFBcpWBHBVUdbxdoKRVysCJ1FxIoSqT4eMoeuNJ03mffN\nm/nt9wOBTDJMHkq+fWfeDDNOIgA1XdH3AADdIXCgMAIHCiNwoDACBwojcKAwAgcKI3CgMAIHCvtL\nFze6devWLCwsdHHTrfv555/7nrAqp0+f7nvCqszSMyV37tzZ94TGRqORzp8/70nX6yTwhYUFDYfD\nLm66dcePH+97wqrs2bOn7wmrsrS01PeExh5//PG+JzT2yCOPNLoed9GBwggcKIzAgcIIHCiMwIHC\nCBwojMCBwggcKIzAgcIIHCiMwIHCCBwojMCBwggcKIzAgcIIHCisUeC299v+0vYZ2w91PQpAOyYG\nbntO0tOSbpa0S9Kdtnd1PQzA9JocwXdLOpPkqyS/SnpF0u3dzgLQhiaBb5f0zUWXz46/BmCDa+0k\nm+2Dtoe2h6PRqK2bBTCFJoF/K2nHRZfnx1/7H0kOJ1lMsjgYDNraB2AKTQL/SNI1tnfavlLSHZLe\n6HYWgDZMfF30JBds3yfpLUlzkp5LcrLzZQCm1uiND5K8KenNjrcAaBnPZAMKI3CgMAIHCiNwoDAC\nBwojcKAwAgcKI3CgMAIHCiNwoDACBwojcKAwAgcKI3CgMAIHCiNwoDACBwpr9Ioulf3yyy99T1iV\npaWlviesyrZt2/qe0NiBAwf6ntDYE0880eh6HMGBwggcKIzAgcIIHCiMwIHCCBwojMCBwggcKIzA\ngcIIHCiMwIHCCBwojMCBwggcKIzAgcIIHCiMwIHCJgZu+znb52x/vh6DALSnyRH8BUn7O94BoAMT\nA0/yrqTv12ELgJbxGBworLXAbR+0PbQ9HI1Gbd0sgCm0FniSw0kWkywOBoO2bhbAFLiLDhTW5M9k\nL0t6X9K1ts/avrf7WQDaMPGdTZLcuR5DALSPu+hAYQQOFEbgQGEEDhRG4EBhBA4URuBAYQQOFEbg\nQGEEDhRG4EBhBA4URuBAYQQOFEbgQGEEDhQ28QUfgGls3ry57wmNbdmype8JjV1xRbNjM0dwoDAC\nBwojcKAwAgcKI3CgMAIHCiNwoDACBwojcKAwAgcKI3CgMAIHCiNwoDACBwojcKAwAgcKI3CgsImB\n295h+5jtL2yftH1oPYYBmF6Tl2y6IOmBJJ/Y/qukj20fTfJFx9sATGniETzJd0k+GX9+XtIpSdu7\nHgZgeqt6DG57QdINkj7oYgyAdjUO3PYWSa9Juj/Jj5f4/kHbQ9vD0WjU5kYAa9QocNubtBz3S0le\nv9R1khxOsphkcTAYtLkRwBo1OYtuSc9KOpXkye4nAWhLkyP4Xkl3S9pn+8T445aOdwFowcQ/kyV5\nT5LXYQuAlvFMNqAwAgcKI3CgMAIHCiNwoDACBwojcKAwAgcKI3CgMAIHCiNwoDACBwojcKAwAgcK\nI3CgMAIHCiNwoLAmb3wArNk999zT94T/axzBgcIIHCiMwIHCCBwojMCBwggcKIzAgcIIHCiMwIHC\nCBwojMCBwggcKIzAgcIIHCiMwIHCCBwojMCBwiYGbnuz7Q9tf2r7pO3H1mMYgOk1ecmmJUn7kvxk\ne5Ok92z/K8m/O94GYEoTA08SST+NL24af6TLUQDa0egxuO052ycknZN0NMkH3c4C0IZGgSf5Lcn1\nkuYl7bZ93Z+vY/ug7aHt4Wg0ansngDVY1Vn0JD9IOiZp/yW+dzjJYpLFwWDQ1j4AU2hyFn1g++rx\n51dJuknS6a6HAZhek7Po2yT90/aclv9DeDXJkW5nAWhDk7Pon0m6YR22AGgZz2QDCiNwoDACBwoj\ncKAwAgcKI3CgMAIHCiNwoDACBwojcKAwAgcKI3CgMAIHCiNwoDACBwojcKCwJq/oUtryq0LPjlnb\n+/zzz/c9obFHH3207wmt4wgOFEbgQGEEDhRG4EBhBA4URuBAYQQOFEbgQGEEDhRG4EBhBA4URuBA\nYQQOFEbgQGEEDhRG4EBhBA4U1jhw23O2j9s+0uUgAO1ZzRH8kKRTXQ0B0L5Ggduel3SrpGe6nQOg\nTU2P4E9JelDS7x1uAdCyiYHbPiDpXJKPJ1zvoO2h7eFoNGptIIC1a3IE3yvpNttfS3pF0j7bL/75\nSkkOJ1lMsjgYDFqeCWAtJgae5OEk80kWJN0h6e0kd3W+DMDU+Ds4UNiq3tkkyTuS3ulkCYDWcQQH\nCiNwoDACBwojcKAwAgcKI3CgMAIHCiNwoDACBwojcKAwAgcKI3CgMAIHCiNwoDACBwojcKAwAgcK\nc5L2b9QeSfpPyze7VdJ/W77NLs3S3lnaKs3W3q62/i3JxFc37STwLtgeJlnse0dTs7R3lrZKs7W3\n763cRQcKI3CgsFkK/HDfA1ZplvbO0lZptvb2unVmHoMDWL1ZOoIDWKWZCNz2fttf2j5j+6G+91yO\n7edsn7P9ed9bJrG9w/Yx21/YPmn7UN+bVmJ7s+0PbX863vpY35uasD1n+7jtI338/A0fuO05SU9L\nulnSLkl32t7V76rLekHS/r5HNHRB0gNJdkm6UdI/NvC/7ZKkfUn+Lul6Sftt39jzpiYOSTrV1w/f\n8IFL2i3pTJKvkvyq5Xc4vb3nTStK8q6k7/ve0USS75J8Mv78vJZ/Ebf3u+rSsuyn8cVN448NfQLJ\n9rykWyU909eGWQh8u6RvLrp8Vhv0l3CW2V6QdIOkD/pdsrLx3d0Tks5JOppkw24de0rSg5J+72vA\nLASOjtneIuk1Sfcn+bHvPStJ8luS6yXNS9pt+7q+N63E9gFJ55J83OeOWQj8W0k7Lro8P/4aWmB7\nk5bjfinJ633vaSLJD5KOaWOf69gr6TbbX2v5YeU+2y+u94hZCPwjSdfY3mn7Skl3SHqj500l2Lak\nZyWdSvJk33sux/bA9tXjz6+SdJOk0/2uWlmSh5PMJ1nQ8u/s20nuWu8dGz7wJBck3SfpLS2fBHo1\nycl+V63M9suS3pd0re2ztu/te9Nl7JV0t5aPLifGH7f0PWoF2yQds/2Zlv/TP5qklz89zRKeyQYU\ntuGP4ADWjsCBwggcKIzAgcIIHCiMwIHCCBwojMCBwv4APqD4Xdwde0AAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f03cd91f630>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "And the whole sample:\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nSUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f03b61a2ef0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_train [shape (50000,)] 10 samples:\n",
      " [5 0 4 1 9 2 1 3 1 4]\n"
     ]
    }
   ],
   "source": [
    "# X contains rgb values divided by 255\n",
    "print(\"X_train [shape %s] sample patch:\\n\" % (str(X_train.shape)), X_train[1, 15:20, 5:10])\n",
    "print(\"A closeup of a sample patch:\")\n",
    "plt.imshow(X_train[1, 15:20, 5:10], cmap=\"Greys\")\n",
    "plt.show()\n",
    "print(\"And the whole sample:\")\n",
    "plt.imshow(X_train[1], cmap=\"Greys\")\n",
    "plt.show()\n",
    "print(\"y_train [shape %s] 10 samples:\\n\" % (str(y_train.shape)), y_train[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000, 784)\n",
      "(10000, 784)\n"
     ]
    }
   ],
   "source": [
    "# flatten images\n",
    "X_train_flat = X_train.reshape((X_train.shape[0], -1))\n",
    "print(X_train_flat.shape)\n",
    "\n",
    "X_val_flat = X_val.reshape((X_val.shape[0], -1))\n",
    "print(X_val_flat.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000, 10)\n",
      "[[ 0.  0.  0.  0.  0.  1.  0.  0.  0.  0.]\n",
      " [ 1.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  1.  0.  0.  0.  0.  0.]] [5 0 4]\n"
     ]
    }
   ],
   "source": [
    "# one-hot encode the target\n",
    "y_train_oh = keras.utils.to_categorical(y_train, 10)\n",
    "y_val_oh = keras.utils.to_categorical(y_val, 10)\n",
    "\n",
    "print(y_train_oh.shape)\n",
    "print(y_train_oh[:3], y_train[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# building a model with keras\n",
    "from keras.layers import Dense, Activation\n",
    "from keras.models import Sequential\n",
    "\n",
    "# we still need to clear a graph though\n",
    "s = reset_tf_session()\n",
    "\n",
    "model = Sequential()  # it is a feed-forward network without loops like in RNN\n",
    "model.add(Dense(256, input_shape=(784,)))  # the first layer must specify the input shape (replacing placeholders)\n",
    "model.add(Activation('sigmoid'))\n",
    "model.add(Dense(256))\n",
    "model.add(Activation('sigmoid'))\n",
    "model.add(Dense(10))\n",
    "model.add(Activation('softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 256)               200960    \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 10)                2570      \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 10)                0         \n",
      "=================================================================\n",
      "Total params: 269,322\n",
      "Trainable params: 269,322\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# you can look at all layers and parameter count\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# now we \"compile\" the model specifying the loss and optimizer\n",
    "model.compile(\n",
    "    loss='categorical_crossentropy', # this is our cross-entropy\n",
    "    optimizer='adam',\n",
    "    metrics=['accuracy']  # report accuracy during training\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1/40\n",
      "\n",
      "\n",
      "Epoch 2/40\n",
      "\n",
      "\n",
      "Epoch 3/40\n",
      "\n",
      "\n",
      "Epoch 4/40\n",
      "\n",
      "\n",
      "Epoch 5/40\n",
      "\n",
      "\n",
      "Epoch 6/40\n",
      "\n",
      "\n",
      "Epoch 7/40\n",
      "\n",
      "\n",
      "Epoch 8/40\n",
      "\n",
      "\n",
      "Epoch 9/40\n",
      "\n",
      "\n",
      "Epoch 10/40\n",
      "\n",
      "\n",
      "Epoch 11/40\n",
      "\n",
      "\n",
      "Epoch 12/40\n",
      "\n",
      "\n",
      "Epoch 13/40\n",
      "\n",
      "\n",
      "Epoch 14/40\n",
      "\n",
      "\n",
      "Epoch 15/40\n",
      "\n",
      "\n",
      "Epoch 16/40\n",
      "\n",
      "\n",
      "Epoch 17/40\n",
      "\n",
      "\n",
      "Epoch 18/40\n",
      "\n",
      "\n",
      "Epoch 19/40\n",
      "\n",
      "\n",
      "Epoch 20/40\n",
      "\n",
      "\n",
      "Epoch 21/40\n",
      "\n",
      "\n",
      "Epoch 22/40\n",
      "\n",
      "\n",
      "Epoch 23/40\n",
      "\n",
      "\n",
      "Epoch 24/40\n",
      "\n",
      "\n",
      "Epoch 25/40\n",
      "\n",
      "\n",
      "Epoch 26/40\n",
      "\n",
      "\n",
      "Epoch 27/40\n",
      "\n",
      "\n",
      "Epoch 28/40\n",
      "\n",
      "\n",
      "Epoch 29/40\n",
      "\n",
      "\n",
      "Epoch 30/40\n",
      "\n",
      "\n",
      "Epoch 31/40\n",
      "\n",
      "\n",
      "Epoch 32/40\n",
      "\n",
      "\n",
      "Epoch 33/40\n",
      "\n",
      "\n",
      "Epoch 34/40\n",
      "\n",
      "\n",
      "Epoch 35/40\n",
      "\n",
      "\n",
      "Epoch 36/40\n",
      "\n",
      "\n",
      "Epoch 37/40\n",
      "\n",
      "\n",
      "Epoch 38/40\n",
      "\n",
      "\n",
      "Epoch 39/40\n",
      "\n",
      "\n",
      "Epoch 40/40\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f03b611a470>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# and now we can fit the model with model.fit()\n",
    "# and we don't have to write loops and batching manually as in TensorFlow\n",
    "model.fit(\n",
    "    X_train_flat, \n",
    "    y_train_oh,\n",
    "    batch_size=512, \n",
    "    epochs=40,\n",
    "    validation_data=(X_val_flat, y_val_oh),\n",
    "    callbacks=[keras_utils.TqdmProgressCallback()],\n",
    "    verbose=0\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Here're the notes for those who want to play around here\n",
    "\n",
    "Here are some tips on what you could do:\n",
    "\n",
    " * __Network size__\n",
    "   * More neurons, \n",
    "   * More layers, ([docs](https://keras.io/))\n",
    "\n",
    "   * Other nonlinearities in the hidden layers\n",
    "     * tanh, relu, leaky relu, etc\n",
    "   * Larger networks may take more epochs to train, so don't discard your net just because it could didn't beat the baseline in 5 epochs.\n",
    "\n",
    "\n",
    " * __Early Stopping__\n",
    "   * Training for 100 epochs regardless of anything is probably a bad idea.\n",
    "   * Some networks converge over 5 epochs, others - over 500.\n",
    "   * Way to go: stop when validation score is 10 iterations past maximum\n",
    "     \n",
    "\n",
    " * __Faster optimization__\n",
    "   * rmsprop, nesterov_momentum, adam, adagrad and so on.\n",
    "     * Converge faster and sometimes reach better optima\n",
    "     * It might make sense to tweak learning rate/momentum, other learning parameters, batch size and number of epochs\n",
    "\n",
    "\n",
    " * __Regularize__ to prevent overfitting\n",
    "   * Add some L2 weight norm to the loss function, theano will do the rest\n",
    "     * Can be done manually or via - https://keras.io/regularizers/\n",
    "   \n",
    "   \n",
    " * __Data augmemntation__ - getting 5x as large dataset for free is a great deal\n",
    "   * https://keras.io/preprocessing/image/\n",
    "   * Zoom-in+slice = move\n",
    "   * Rotate+zoom(to remove black stripes)\n",
    "   * any other perturbations\n",
    "   * Simple way to do that (if you have PIL/Image): \n",
    "     * ```from scipy.misc import imrotate,imresize```\n",
    "     * and a few slicing\n",
    "   * Stay realistic. There's usually no point in flipping dogs upside down as that is not the way you usually see them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  },
  "widgets": {
   "state": {
    "057fbca803ba4eeb8a5cc24540b7be5e": {
     "views": [
      {
       "cell_index": 11
      }
     ]
    },
    "0695d4d2210e4f0b8f278b314dd503fd": {
     "views": [
      {
       "cell_index": 11
      }
     ]
    },
    "0bd9f7afe73942eb87ec585170f5da17": {
     "views": [
      {
       "cell_index": 11
      }
     ]
    },
    "10bf34fa0dbc4261a580a7a9b25abc73": {
     "views": [
      {
       "cell_index": 11
      }
     ]
    },
    "16fd8c23adb24785bbec42f365b49275": {
     "views": [
      {
       "cell_index": 11
      }
     ]
    },
    "2b89c0c7e5424f9f9fa9b8bb4438f30a": {
     "views": [
      {
       "cell_index": 11
      }
     ]
    },
    "2eb76bf8a1764fddbbbfc8268c6646d6": {
     "views": [
      {
       "cell_index": 11
      }
     ]
    },
    "36d604b6410f4e0582c5a0666f060a1b": {
     "views": [
      {
       "cell_index": 11
      }
     ]
    },
    "38a1ebc885c94d4695dcb31432338b5d": {
     "views": [
      {
       "cell_index": 11
      }
     ]
    },
    "3e3f2f79d9de485e8b911388d08a6716": {
     "views": [
      {
       "cell_index": 11
      }
     ]
    },
    "43825fe860534604a23a9c4db4d31d6c": {
     "views": [
      {
       "cell_index": 11
      }
     ]
    },
    "45b1dcef30d24be1aabab36d647aa4e9": {
     "views": [
      {
       "cell_index": 11
      }
     ]
    },
    "5b618e0dfc3e40ea8aa140d20ecf4d89": {
     "views": [
      {
       "cell_index": 11
      }
     ]
    },
    "68faef3a286e4a9c819b6f180a8e6ee3": {
     "views": [
      {
       "cell_index": 11
      }
     ]
    },
    "6c574c0045cf424885beca0ee59233ae": {
     "views": [
      {
       "cell_index": 11
      }
     ]
    },
    "7496ae4837664f7285d34d4bdccb3887": {
     "views": [
      {
       "cell_index": 11
      }
     ]
    },
    "802e6bc12db54ea0b99254d3d8138336": {
     "views": [
      {
       "cell_index": 11
      }
     ]
    },
    "8624806e1ed2405f8a55fecf9f4d415e": {
     "views": [
      {
       "cell_index": 11
      }
     ]
    },
    "888a5acf980b435ea0a0005f0af5f1b7": {
     "views": [
      {
       "cell_index": 11
      }
     ]
    },
    "89c94a5b035248aa840f5eef88cb0782": {
     "views": [
      {
       "cell_index": 11
      }
     ]
    },
    "8f8157b9632c45b78b58a79856c94eee": {
     "views": [
      {
       "cell_index": 11
      }
     ]
    },
    "940bb8f138c84d538814083e2d4bd920": {
     "views": [
      {
       "cell_index": 11
      }
     ]
    },
    "972a1c0e9ec042f6969c390f6fae965c": {
     "views": [
      {
       "cell_index": 11
      }
     ]
    },
    "986c7b0ee9274d14bcdde9bac7541dd0": {
     "views": [
      {
       "cell_index": 11
      }
     ]
    },
    "9c40a62eda7440618ae84d2ccd97bee0": {
     "views": [
      {
       "cell_index": 11
      }
     ]
    },
    "9f278b8051b4486e9cd4e09ed92c41bd": {
     "views": [
      {
       "cell_index": 11
      }
     ]
    },
    "a4e5c5f7af91412c853a4ebec6b1d7ec": {
     "views": [
      {
       "cell_index": 11
      }
     ]
    },
    "a576d67392654f5e835f8617c3c6b2af": {
     "views": [
      {
       "cell_index": 11
      }
     ]
    },
    "ad01c76a434148938b97e3cafd17295c": {
     "views": [
      {
       "cell_index": 11
      }
     ]
    },
    "ad99074511c4411eb6abdecbee9ebf52": {
     "views": [
      {
       "cell_index": 11
      }
     ]
    },
    "af1c32944f40445abe68c41d3659e483": {
     "views": [
      {
       "cell_index": 11
      }
     ]
    },
    "b31492444ede4e40ac1df1b2132dd0e2": {
     "views": [
      {
       "cell_index": 11
      }
     ]
    },
    "b66b38a8c9eb4a97a7a11bd74f2b357f": {
     "views": [
      {
       "cell_index": 11
      }
     ]
    },
    "c34a1883f73348eea3969af8e1ec2ced": {
     "views": [
      {
       "cell_index": 11
      }
     ]
    },
    "c595935401054f22b403db2eb22bc4d7": {
     "views": [
      {
       "cell_index": 11
      }
     ]
    },
    "cc5caf3200214d1facbede4ec820311c": {
     "views": [
      {
       "cell_index": 11
      }
     ]
    },
    "d462943a514841c7ad6b3fd46a8dcfd0": {
     "views": [
      {
       "cell_index": 11
      }
     ]
    },
    "d9edcc95cd884642870623fed747c69f": {
     "views": [
      {
       "cell_index": 11
      }
     ]
    },
    "e700b2f1b0e643928ff10908edfb3b60": {
     "views": [
      {
       "cell_index": 11
      }
     ]
    },
    "fe7c6616691647e08d9db1d009f6f796": {
     "views": [
      {
       "cell_index": 11
      }
     ]
    }
   },
   "version": "1.2.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
